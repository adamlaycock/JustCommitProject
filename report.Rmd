---
title: "Weather in the United States (1996-2023)"
author: "by Just Commit (Group number: 17): Olivia Harris, Maxwell Pohlmann, Sarah Stewart, Helen Miller, Andrew Morris, & Adam Laycock"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-lib}
library(tidyverse)
library(tidymodels)
library(janitor) #converts variable names to snake_case
```

```{r load-data}
details = read_csv('data/original_data/details_combined.csv')
fatalities = read_csv('data/original_data/fatalities_combined.csv')
```

```{r Data Cleaning}
# Merge and convert formats of begin-date variables
details <- details %>%
  # Combine date elements
  unite("BEGIN_DATE", BEGIN_YEARMONTH, BEGIN_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    BEGIN_DATE = as.character(BEGIN_DATE),
    # Add leading zero if the time is 3 digits
    BEGIN_TIME = case_when(
      BEGIN_TIME < 1000 ~ sprintf("%04d", BEGIN_TIME),
      TRUE ~ as.character(BEGIN_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("BEGIN_DT", BEGIN_DATE, BEGIN_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    BEGIN_DT = ymd_hm(BEGIN_DT)
  )

# Merge and convert formats of end-date variables
details <- details %>%
  # Combine date elements
  unite("END_DATE", END_YEARMONTH, END_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    END_DATE = as.character(END_DATE),
    # Add leading zero if the time is 3 digits
    END_TIME = case_when(
      END_TIME < 1000 ~ sprintf("%04d", END_TIME),
      TRUE ~ as.character(END_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("END_DT", END_DATE, END_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    END_DT = ymd_hm(END_DT)
  )

# Remove unnecessary columns
details <- details %>%
  select(
    -c(YEAR, MONTH_NAME, BEGIN_DATE_TIME, END_DATE_TIME, 
       EVENT_NARRATIVE, EPISODE_NARRATIVE
    )
  )

# Rename incorrect column names
details <- details %>%
  rename(
    REGION = STATE,
    REGION_FIPS = STATE_FIPS
  )

# Create duration variable 
details <- details %>% 
  mutate(
    duration = END_DT - BEGIN_DT
  ) %>% 
    mutate(
      duration = as.numeric(duration) / 60
    )

# Change cost suffixes
details <- details %>% 
  mutate(
    DAMAGE_PROPERTY = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_PROPERTY))),
    DAMAGE_CROPS = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_CROPS)))
  )

# Create a total damage column using crops and property
details <- details %>%
  mutate(
    DAMAGE_TOTAL = DAMAGE_PROPERTY + DAMAGE_CROPS
  )

# Remove all records pertaining to an incomplete year
details <- details %>% 
  filter(
    !year(BEGIN_DT) == '2024'
  )

# Use janitor to convert variable names to snakecase
details <- details %>% 
  clean_names()

# Tidy the fatalities DataFrame
fatalities <- fatalities %>% 
  select(
    -FAT_TIME, -FATALITY_DATE, -EVENT_YEARMONTH
  ) %>% 
    mutate(
      YMD = ymd(paste(FAT_YEARMONTH, FAT_DAY))
    ) %>% 
      select(
        -FAT_DAY, -FAT_YEARMONTH
      )

# Remove 2024 records
fatalities <- fatalities %>%
  filter(
    !year(YMD) == '2024'
  )

# Use janitor to convert variables to snakecase
fatalities <- fatalities %>% 
  clean_names()

details <- details %>%
  mutate(
    event_type = recode(
      event_type,
      "Hurricane (Typhoon)" = "Hurricane"
    )
  )

#write_rds(details, "data/clean_data/details_clean.rds", compress = "gz")
#write_rds(fatalities, "data/clean_data/fatalities_clean.rds")
```

## Research Question

How have storm events changed over time, and what are the personal and monetary effects of these changes?

We chose to investigate this question due to the increasingly dramatic shifts in climate observed in recent years.
Many of these changes seem to be significantly impacting the United States, and we wanted to explore those impacts in our data.

## Data

Our data was sourced from the Storm Events Database of the National Oceanic and Atmospheric Administration (NOAA).
The NOAA collects data on weather events in the United States.
This includes information on event location, 50 variables related to event details (including, notably, time of occurance and damages), and 10 variables related to event fatalities.
In our analysis, we focused on over 1.6 million observations collected between 1996 and 2023.

In our initial cleaning, we notably removed unwanted variables such as event narrative (a multi-sentence description of the event), merged date columns and converted them to date-times with `lubridate,` and added a column for total damage cost (summing crop and property damage).
All damage information was converted from a string to a double.
The cleaning script can be found in it's entirety in r `cleaning_script`.

```{r cleaning_script}
# Merge and convert formats of begin-date variables
details <- details %>%
  # Combine date elements
  unite("BEGIN_DATE", BEGIN_YEARMONTH, BEGIN_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    BEGIN_DATE = as.character(BEGIN_DATE),
    # Add leading zero if the time is 3 digits
    BEGIN_TIME = case_when(
      BEGIN_TIME < 1000 ~ sprintf("%04d", BEGIN_TIME),
      TRUE ~ as.character(BEGIN_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("BEGIN_DT", BEGIN_DATE, BEGIN_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    BEGIN_DT = ymd_hm(BEGIN_DT)
  )

# Merge and convert formats of end-date variables
details <- details %>%
  # Combine date elements
  unite("END_DATE", END_YEARMONTH, END_DAY, sep = "") %>% 
  mutate(
    # Convert date to a character for union with time
    END_DATE = as.character(END_DATE),
    # Add leading zero if the time is 3 digits
    END_TIME = case_when(
      END_TIME < 1000 ~ sprintf("%04d", END_TIME),
      TRUE ~ as.character(END_TIME)
    )
  ) %>%
  # Unite date and time into one string
  unite("END_DT", END_DATE, END_TIME, sep = " ") %>%
  mutate(
    # Convert to a datetime
    END_DT = ymd_hm(END_DT)
  )

# Remove unnecessary columns
details <- details %>%
  select(
    -c(YEAR, MONTH_NAME, BEGIN_DATE_TIME, END_DATE_TIME, 
       EVENT_NARRATIVE, EPISODE_NARRATIVE
    )
  )

# Rename incorrect column names
details <- details %>%
  rename(
    REGION = STATE,
    REGION_FIPS = STATE_FIPS
  )

# Create duration variable 
details <- details %>% 
  mutate(
    duration = END_DT - BEGIN_DT
  ) %>% 
    mutate(
      duration = as.numeric(duration) / 60
    )

# Change cost suffixes
details <- details %>% 
  mutate(
    DAMAGE_PROPERTY = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_PROPERTY))),
    DAMAGE_CROPS = as.numeric(gsub("K", "e+03", gsub("M", "e+06", DAMAGE_CROPS)))
  )

# Create a total damage column using crops and property
details <- details %>%
  mutate(
    DAMAGE_TOTAL = DAMAGE_PROPERTY + DAMAGE_CROPS
  )

# Remove all records pertaining to an incomplete year
details <- details %>% 
  filter(
    !year(BEGIN_DT) == '2024'
  )

# Use janitor to convert variable names to snakecase
details <- details %>% 
  clean_names()

# Tidy the fatalities DataFrame
fatalities <- fatalities %>% 
  select(
    -FAT_TIME, -FATALITY_DATE, -EVENT_YEARMONTH
  ) %>% 
    mutate(
      YMD = ymd(paste(FAT_YEARMONTH, FAT_DAY))
    ) %>% 
      select(
        -FAT_DAY, -FAT_YEARMONTH
      )

# Remove 2024 records
fatalities <- fatalities %>%
  filter(
    !year(YMD) == '2024'
  )

# Use janitor to convert variables to snakecase
fatalities <- fatalities %>% 
  clean_names()

details <- details %>%
  mutate(
    event_type = recode(
      event_type,
      "Hurricane (Typhoon)" = "Hurricane"
    )
  )

#save cleaned data
#write_rds(details, "../clean_data/details_clean.rds", compress = "gz")
#write_rds(fatalities, "../clean_data/fatalities_clean.rds")
```





```{r load-data}
details = read_rds('../../data/clean_data/details_clean.rds')
fatalities = read_rds('../../data/clean_data/fatalities_clean.rds')
```



## Changes in Storm Event Frequency Over Time

To identify how storm events have changed over time we used a linear regression model attempting to predict the number of storm events per year.
We grouped event frequencies by year to balance statistical significance and trend visibility as smaller groups were skewed by outliers, while larger ones (e.g., 5-year periods) lacked sufficient detail to show trends over time.
To do this, we created the variable events_per_year by summing event frequencies annually and ran the regression: lm(event_count ~ year, data = events_per_year).
The results predicted for every one-year increase there will be, on average, an additional 774.2 storm events per year.
These results are statistically significant (p > 0.01, R-squared = 0.5358) with over half of the variation in event count per year explained by the variable year.

We created similar models predicting fatalities and damages per year.
These models predict that for every additional year, on average, fatalities will increase by 15.3 and damages will increase by $447 million.
Both these results have p-values < 0.05 and are statistically significant.
However, the R-squared values are low (0.3 for damages and 0.2 for fatalities), indicating there are other factors with a significant influence.







```{r filter events}
severe_details <- details %>%
  select(-c(episode_id, wfo, source, region_fips, end_dt, region_fips, cz_type, cz_fips, cz_name, cz_timezone, injuries_direct, injuries_indirect, deaths_direct, deaths_indirect, flood_cause, category, tor_f_scale, tor_length, tor_width, tor_other_wfo, tor_other_cz_state, tor_other_cz_fips, tor_other_cz_name, begin_azimuth, end_azimuth, begin_lon, begin_lat, end_lon, end_lat, duration, magnitude_type, begin_range, begin_location, end_range)) %>%
  group_by(event_type) %>%
  mutate(sum_damage_total = sum(damage_total, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(
    desc(sum_damage_total),
  ) %>%
  distinct(event_type, .keep_all = TRUE) 

```

```{r not severe}
not_severe_details <- details %>%
  select(-c(episode_id, wfo, source, region_fips, end_dt, region_fips, cz_type, cz_fips, cz_name, cz_timezone, injuries_direct, injuries_indirect, deaths_direct, deaths_indirect, flood_cause, category, tor_f_scale, tor_length, tor_width, tor_other_wfo, tor_other_cz_state, tor_other_cz_fips, tor_other_cz_name, begin_azimuth, end_azimuth, begin_lon, begin_lat, end_lon, end_lat, duration, magnitude_type, begin_range, begin_location, end_range)) %>%
  group_by(event_type) %>%
  mutate(sum_damage_total = sum(damage_total, na.rm = TRUE)) %>%
  ungroup() %>%
  arrange(
    desc(sum_damage_total),
  ) 
```

```{r joining}
combined_data <- severe_details %>%
  full_join(fatalities, by = "event_id")
```
 
```{r not combined}
not_combined_data <- not_severe_details %>%
  full_join(fatalities, by = "event_id")
```

When analyzing the total number of weather events, we found Texas experienced significantly more weather events than any other state. This could potentially be explained by its considerably large size. 

```{r not severe frequency events}
not_frequency_by_region <-not_combined_data%>%
  group_by(region) %>%
  summarize(
    no_events = n_distinct(event_id)) %>%
  mutate(no_events = no_events) %>%
  ungroup()
 
not_total_events <- not_combined_data %>%
  summarize(total_events = n_distinct(event_id)) %>%
  pull(total_events)
  
not_frequency_by_region <- not_frequency_by_region %>%
  mutate(not_total_events= total_events)
 
not_combined_data <- not_combined_data %>%
  left_join(not_frequency_by_region, by="region")
 
```
 
```{r not severe event frequency map}
not_combined_data <- not_combined_data %>% 
  mutate(region = tolower(region))
 
not_frequency_by_region <- not_frequency_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(not_frequency_by_region, by = "region") 
 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_events)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of events") + 
  labs(
    title = "Number of all weather events experienced per state") +
  theme_minimal()
```

With the filtered data, analyzed which states experienced severe weather events and how often. From this we found Florida, Texas and California endured the highest frequency of severe weather events. 

```{r  frequency events}
frequency_by_region <-combined_data%>%
  group_by(region) %>%
  summarize(
    no_events = n_distinct(event_id)) %>%
  mutate(no_events = no_events) %>%
  ungroup()
 
total_events <- combined_data %>%
  summarize(total_events = n_distinct(event_id)) %>%
  pull(total_events)
  
frequency_by_region <- frequency_by_region %>%
  mutate(total_events= total_events)
 
combined_data <- combined_data %>%
  left_join(frequency_by_region, by="region")
 
```
 
```{r event frequency map}
 
combined_data <- combined_data %>% 
  mutate(region = tolower(region))
 
frequency_by_region <- frequency_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(frequency_by_region, by = "region") 
 
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_events)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of severe events") + 
  labs(
    title = "Map of the USA",
    subtitle = "Most severe weather events experienced per state") +
  theme_minimal()
```

When comparing this data to the total number of fatalities caused by all weather events, we found that, as predicted, Florida, California, and Texas all experienced a high fatality rate. However, unexpectedly Arizona showed a high number of fatalities, even though they did not experience a lot of severe weather. 

```{r not fatalities  map}
 
not_combined_data <- not_combined_data %>% 
  mutate(region = tolower(region))
 
fatalities_by_region <- fatalities_by_region %>%
  mutate(region = tolower(region))
 
             
usa_map <- map_data("state")
             
map_combined <- usa_map %>% 
  left_join(fatalities_by_region, by = "region") 
 
map_combined
 
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = no_fatalities)
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Number of fatalities") + 
  labs(
    title = "Number of fatalities per state") +
  theme_minimal()
```

Finally, we compared the results found to the total damages caused by severe weather events and much of what we discovered was to be expected, with Florida, Texas, California and Arizona all having experienced a high degree of total damages caused by severe weather events. However, many states experienced similar high damage costs while experiencing drastically less frequent severe weather events. From this we concluded that it could potentially stem from the lack of sufficient infrastructure in place to prepare for storm events, especially compared to states experiencing severe weather events frequently. 

```{r map damage}
 
#load map data
combined_data <- combined_data %>% 
      mutate(region = tolower(region))
             
usa_map <- map_data("state")     
             
map_combined<- combined_data%>% 
  full_join(usa_map)
 
# Plot the map
map_combined %>% 
  ggplot(
    mapping = aes(
      x = long, y = lat, group = group, fill = log(sum_damage_total)
    )
  ) +
  geom_polygon() +
  scale_fill_gradient(low = "yellow", high = "red", name = "Total Damage ($)") + 
  labs(title = "States experiencing the most damage") +
  theme_minimal()
```










# Damage Makeup by Event Type

```{r fig.align='center'}
cost_data <- details %>% 
  select(
    event_type, damage_property, damage_crops
  ) %>% 
    group_by(
      event_type
    ) %>% 
      summarise(
        Property = sum(
          damage_property, 
          na.rm=TRUE
        ),
        Crops = sum(
          damage_crops, 
          na.rm=TRUE
        )
      ) %>%
        slice_max(
          order_by = Property + Crops, 
          n = 20
        ) %>% 
          pivot_longer(
            cols=c('Property', 'Crops'),
            names_to='type',
            values_to='damage'
          )

cost_data %>% 
  ggplot(
    mapping=aes(
      x=damage/sum(damage)*100,
      y=reorder(event_type, damage),
      fill=type
    )
  ) +
  geom_col() +
  labs(
    title = 'Damage by Storm Event Type between 1996 & 2023', 
    subtitle = 'Total Damage (Property + Crops) was $227bn',
    y = 'Type of Event',
    x = 'Pecentage of Total Damage',
    fill='Damage Type',
    caption='Only the 20 most damaging event types are shown.'
  ) +
  theme_minimal() +
  theme(
    plot.caption = element_text(hjust = -0.75, face= "italic"),
    aspect.ratio=4/4
  )
```
This plot shows that not all highly damaging events caused the same types of damages. Flash floods and tornadoes predominantly caused damage to property, while droughts and freezes disproportionately affected crops.

# Event Severity Clustering Using K-means
## Preprocessing & Optimisation
### Feature Selection & Engineering
```{r Feature Selection & Engineering}
# Create combined metrics by combining direct and indirect
details <- details %>% 
  mutate(
    attributed_deaths = deaths_direct + deaths_indirect,
    attributed_injuries = injuries_direct + injuries_indirect,
    attributed_casualties = attributed_deaths + attributed_injuries
  )

# Select only relevant data and remove rows with NA
model_data <- details %>% 
  select(
    event_id, attributed_casualties, damage_total, event_type
  ) %>% 
    filter(
      !is.na(damage_total) & !is.na(attributed_casualties)
    )

# Z-Score normalise the data
model_data_scaled <- model_data %>%
  mutate(
    damage_total = as.vector(scale(damage_total)),
    attributed_casualties = as.vector(scale(attributed_casualties))
)
```

### K Hyperparameter Optimisation
```{r Cluster number optimisation, fig.align='center', eval=FALSE}
# Initialise empty lists
k_list = list()
sse_list = list()

# Loop through 1-30 clusters and store total sum of squared error
for (k in 1:30) {
  kmeans_spec <- k_means(num_clusters = k)
  
  kmeans_fit <- kmeans_spec %>%
    fit(~ damage_total + attributed_casualties, data = model_data_scaled)
  
  kmeans_result <- kmeans_fit$fit
  sse <- kmeans_result$tot.withinss
  
  k_list <- c(k_list, k)
  sse_list <- c(sse_list, sse)
}

# Convert lists to a DataFrame
elbow_data <- data.frame(unlist(k_list), unlist(sse_list))
names(elbow_data) = c("k","total_sse")


# Create elbow plot using clustering data
elbow_data %>% 
  ggplot(
    mapping=aes(
      x=k,
      y=total_sse
    )
  ) + 
  geom_line() +
  geom_point() + 
  scale_x_continuous(breaks = seq(1, 30, by = 1)) +
  labs(
    title='Total SSE by Number of Clusters', 
    x='Number of Clusters (k)',
    y='Total SSE'
  ) +
  theme_minimal()
```

## Fitting the Optimised Model
```{r Optimised Model}
# Create a model object using the optimum number of clusters
kmeans_spec <- k_means(num_clusters = 3)

# Fit the model
kmeans_fit <- kmeans_spec %>%
  fit(~ damage_total + attributed_casualties, data = model_data_scaled)

# Add clusters to data
model_data_scaled <- kmeans_fit %>% 
  augment(
    model_data_scaled
  )

# Change cluster names
model_data_scaled <- model_data_scaled %>% 
  rename(
    cluster = .pred_cluster
  ) %>% 
  mutate(
    cluster = case_when(
      cluster == 'Cluster_1' ~ 'Low Damage / Low Casualties',
      cluster == 'Cluster_2' ~ 'High Damage / Low Casualties',
      TRUE ~ 'Low Damage / High Casualties'
    )
  )
```

## Visualising Clustering Results
### Scatterplot using Clusters
```{r Scatter Plot of Scaled Data by Cluster, fig.align='center'}
# Create a scatterplot of the features, hued by cluster
model_data_scaled %>%
  ungroup() %>% 
    ggplot(
      mapping=aes(
        x=damage_total,
        y=attributed_casualties,
        colour=cluster
      )
    ) +
    geom_jitter() +
    labs(
      x='Scaled Total Damage',
      y='Scaled Attributed Casualties',
      title='K-Means Clustering of Casualties & Damages',
      colour='Cluster'
    ) +
    theme_minimal()
```

### Tabular Clusters
```{r Cluster Frequency Table, fig.align='center'}
# Create a table for the number of events in each cluster
model_table <- model_data_scaled %>% 
  group_by(
    cluster
  ) %>% 
    summarise(
      total = n()
    ) %>%
      rename(
        Cluster = cluster,
        Total = total
      ) %>% 
        arrange(
          desc(Total)
        )

kable(model_table)
```

Across the assessed time period, the vast majority of storm events fell into the low severity category with only a relative handful being found in the extremes of high damage or high casualties.

# Geospatial Correlation Between Events & Time
## Building the Function
```{r Map Correlations Between Number of Events & Time}
# Define function
map_correlations <- function(event_type) {
  # Defuse argument for dplyr
  event_type_expr <- enquo(event_type)
  
  # Group by region and year
  total_region_data <- details %>%
    select(
      begin_dt, event_type, region
    ) %>% 
      group_by(
        region, year(begin_dt)
      ) %>%
        summarise(
          ov_total = n()
        ) %>% 
          rename(
            year = `year(begin_dt)`
          )

  # Evaluate and filter by event type, join to total events
  region_data <- details %>%
    filter(
      event_type == !!event_type_expr
    ) %>% 
      select(
        begin_dt, event_type, region
      ) %>% 
        group_by(
          region, year(begin_dt)
        ) %>%
          summarise(
            total = n()
          ) %>% 
            arrange(
              desc(total)
            ) %>% 
              rename(
                year = `year(begin_dt)`
              ) %>% 
                inner_join(
                  total_region_data, by=c('region', 'year')
                ) %>% 
                  mutate(
                    prop = total / ov_total
                  )

  # Run pearson's rank between time & event proportion
  correlation_data <- region_data %>%
    group_by(
      region
    ) %>%
      summarise(
        # Continue through common, non-fatal errors
        corr_test = list(tryCatch(
          cor.test(
            year, 
            prop, 
            method = "pearson", 
            use = "complete.obs"
          ), 
          error = function(e) NULL
        ))
      ) %>%
        # Add correlation and p-values into variables
        mutate(
          corr = sapply(corr_test, function(test) if (!is.null(test)) test$estimate else NA),
          p_value = sapply(corr_test, function(test) if (!is.null(test)) test$p.value else NA)
        ) %>%
          # Remove NA values and statistically insignificant results
          filter(
            !is.na(corr) & !is.na(p_value) & p_value <= 0.05
          ) %>%
            select(
              region, corr, p_value
            ) %>%
              mutate(
                event_type = event_type
              )
  
  # Build a map of the US
  map_data <- usmap::us_map(regions = "states")
  
  # Mutate map_data to have parity with correlation data
  map_data <- map_data %>% 
    mutate(
      full = tolower(full)
    ) %>% 
      rename(
        region = full
      )
  
  # Alter correlation region data
  correlation_data <- correlation_data %>% 
    mutate(
      region = tolower(region)
    )
  
  # Join correlation and map data
  data <- left_join(map_data, correlation_data, by='region')
  
  # Build the map showing correlations
  plot_usmap(
    data=data, 
    values='corr'
  ) +
  scale_fill_continuous(name = "Correlation \nCoefficient") +
  theme(legend.position = "right") +
  labs(
    title=paste('Correlations Between Year &', event_type, 'Events as a Proportion of Total State Events'
          ),
    subtitle = 'Only statistically significant (p<=0.05) correlation coefficients are shown'
  )
}
```

## Using the Function {.panelset}

### Thunderstorm Wind

#### Thunderstorm Wind Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Thunderstorm Wind')
```

### Droughts

#### Drought Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Drought')
```

### Hurricanes

#### Hurricane Events

```{r message=FALSE, warning=FALSE, fig.align='center'}
map_correlations('Hurricane')
```

##
These plots highlight a key limitation with this script, rarer events are much harder to track and predict through time. As a result, lots of the potential correlations for rarer events, such as hurricanes or droughts, are not shown due to the statistical insignificance owing to a smaller sample size.

# Deadly Storm Predictor
## Preprocessing
### Response & Sampling
```{r}
# Create new deady variable based on deaths
details <- details %>% 
  mutate(
    deadly = case_when(
      deaths_direct > 0 | deaths_indirect > 0 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    )
  )

# Select only relevant variables
model_data <- details %>% 
  select(
    begin_dt, region, event_type, deadly
  )

# Take a large sample of the data
model_data <- model_data %>% 
  sample_n(
    1000000
  )
```

### Train, Test Split
```{r}
# Set seed and split data
set.seed(1)
storm_split <- initial_split(model_data)
storm_train <- training(storm_split)
storm_test  <- testing(storm_split)
```

### Building the Recipe, Model, & Workflow
```{r}
# Build preprocessing recipe
storm_rec_1 <- recipe(
  deadly ~ .,
  data = storm_train
) %>% 
  step_dummy(all_nominal(), -all_outcomes())

# Declare model
storm_mod_1 <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")
  
# Build workflow using recipe and model
storm_wflow_1 <- workflow() %>%
  add_recipe(storm_rec_1) %>%
  add_model(storm_mod_1)
```

## Training the Model
### Fitting the Model Using Parallel Processing
```{r message=FALSE, warning=FALSE}
# Use parallel processing to speed up fit
registerDoParallel(cores = detectCores() - 1)

# Fit and store the model
storm_fit_1 <- fit(storm_wflow_1, data = storm_train)
```

### Extracting Prediction Probabilities
```{r message=FALSE, warning=FALSE}
# Create prediction probabilities and bind to test data
storm_pred <- predict(
  storm_fit_1, 
  storm_test, 
  type = "prob"
) %>%
  bind_cols(
    storm_test
  )
```

## Analysing Model Fit
### ROC Curve & AUC Value
```{r fig.align='center'}
# Convert response variable to a factor
storm_pred <- storm_pred %>% 
  mutate(
    deadly = as.factor(deadly)
  )

# Calculate an AUC score
auc_score <- storm_pred %>% 
  roc_auc(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  )

# Build ROC curve and attach AUC score
storm_pred %>%
  roc_curve(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  ) %>%
  autoplot() +
  labs(
    title = 'ROC Curve for Deadly Storm Predictor',
    x = 'False Positive Rate / 1 - Specificity',
    y = 'True Positive Rate / Sensitivity'
  ) +
  geom_text(
    x = 0.6,
    y = 0.45,
    label = paste("AUC = ", round(auc_score$.estimate, 2))
  )
```

### Building a Confusion Matrix
```{r message=FALSE, warning=FALSE, fig.align='center'}
# Use probability cutoff to calculate model prediction
storm_pred <- storm_pred %>% 
  mutate(
    prediction = case_when(
      .pred_Deadly >= 0.5 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    ),
    prediction = as.factor(prediction)
  )

# Construct and a confusion matrix object and convert it to a tibble
conf_mat <- conf_mat(
  data = storm_pred,
  truth = deadly,
  estimate = prediction
) %>%
  tidy()

# Extract values from the tibble to build a new, clean tibble
confusion <- tibble(
  truth = c('Deadly', 'Not Deadly', 'Deadly', 'Not Deadly'),
  prediction = c('Deadly', 'Not Deadly', 'Not Deadly', 'Deadly'),
  n = c(conf_mat[1,2], conf_mat[4,2], conf_mat[2,2], conf_mat[3,2])
)

# Plot a confusion matrix
confusion %>% 
  ggplot(
    mapping = aes(
      x = truth, 
      y = prediction
    )
  ) +
  geom_tile(fill = "steelblue",
            color = "black",
            size = 0.5
  ) +
  geom_text(
    mapping = aes(
      label = n
    ), 
    color = "black", 
    size = 5
  ) +
  theme_minimal() +
  labs(
    title = "Confusion Matrix for Deadly Storm Predictor",
    x = "Model Prediction", 
    y = "Truth"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = 'none'
  )
```

### Accuracy
```{r}
# Calculate an accuracy metric
storm_pred %>% 
  mutate(
    result = case_when(
      deadly == prediction ~ 'Correct',
      TRUE ~ 'Incorrect'
    )
  ) %>% 
    group_by(
      result
    ) %>% 
      summarise(
        percentage = n()/250000*100
      ) %>%
        rename(
        Result = result,
        Percentage = percentage
      ) %>% 
        kable()
```




















































The write-up of your project and findings go here.

Think of this as the text of your presentation with some extra detail to cover what there was not time to discuss in the presentation.
This might include any assumptions you made when doing your analysis, any limitations of the work you have done or any ideas you have for future work.
Feel free to split this section into subsections to make it easier to read.

The length should be roughly 1,500 words.
If you want to use a word count addin, you can install this by copying and pasting the following into RStudio:

`devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)`

You will then need to restart RStudio.
Once you have done that, select the text you want to count the words of, go to Addins, and select the `Word count` addin.
This addin counts words using two different algorithms, but the results should be similar and as long as you're in the ballpark of 1,500 words, you're good!
The addin will ignore code chunks and only count the words in prose.
If you don't want to use the addin you can always copy and paste the text into Microsoft Word to do a Word Count!

You can also load your data here and present any analysis results / plots.
Make sure to hide your code with `echo = FALSE` unless the point you are trying to make is about the code itself, in which case you should show your code.

## Impact of Storm Events

We also chose to explore the impact of weather events across the United States. Initially, unessesary variables were removed from `details_clean.rds`.Then, the data was filtered to find the most severe weather events using `damage_total`, and stored in either `severe_details` and `not_severe_details`, with the latter containing all other weather events. This was then joined to `fatalities_clean.rds`.


Considering the total number of weather events, we found Texas experienced significantly more weather events than any other state.This could potentially be explained by its considerable size.


## References

NOAA Weather service, provided data for the project, accessed at URL: <https://www.ncdc.noaa.gov/stormevents/faq.jsp> NOAA Weather service, data format guide, accessed at URL: <https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/Storm-Data-Bulk-csv-Format.pdf>
