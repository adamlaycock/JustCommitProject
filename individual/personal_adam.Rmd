---
title: "Adam Personal Investigation"
author: "AdamLaycock"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-lib, message = FALSE}
library(tidyverse)
library(janitor)
library(workflows)
library(parsnip)
library(tidyclust)
library(tidymodels)
library(usmap)
library(gghighlight)
```

# Load & Clean Data

```{r load-data}
details = read_rds('../data/clean_data/details_clean.rds')
fatalities = read_rds('../data/clean_data/fatalities_clean.rds')
```

# Damage by Event Type

```{r}
cost_data <- details %>% 
  select(
    event_type, damage_property, damage_crops
  ) %>% 
    group_by(
      event_type
    ) %>% 
      summarise(
        Property = sum(damage_property, na.rm=TRUE),
        Crops = sum(damage_crops, na.rm=TRUE)
      ) %>%
        slice_max(
          order_by = Property + Crops, 
          n = 20
        ) %>% 
          pivot_longer(cols=c('Property', 'Crops'),
                      names_to='type',
                      values_to='damage'
          )

damage_plot <- cost_data %>% 
  ggplot(
    mapping=aes(
      x=damage/sum(damage)*100,
      y=reorder(event_type, damage),
      fill=type
    )
  ) +
  geom_col() +
  labs(title = 'Damage by Storm Event Type between 1996 & 2023', 
       subtitle = 'Total Damage (Property + Crops) was $227bn',
       y = 'Type of Event',
       x = 'Pecentage of Total Damage',
       fill='Damage Type',
       caption='Only the 20 most damaging event types are shown.'
  ) +
  theme_minimal() +
  theme(plot.caption = element_text(hjust = -0.75, face= "italic"))

#ggsave('plots/Damage by Event Type.jpg', plot=damage_plot)
```

# Simple Statistics Function 

```{r Simple Statistics Function}
eval_percentage_criteria <- function(col, type) {
  col <- sym(col)
  
  if (type == 'event') {
    total <- details %>% 
      nrow()
  
    criteria_total <- details %>% 
      filter(
        !!col > 0
      ) %>%
        nrow()
    
    print('Percentage fulfilling criteria by event: ')
    print(criteria_total / total * 100)
  }
  
  else if (type == 'episode') {
    total <- details %>% 
      group_by(
        episode_id
      ) %>% 
        summarise(
          total = sum(!!col)
        ) %>% 
          nrow()
    
    criteria_total <- details %>% 
      group_by(
        episode_id
      ) %>% 
        summarise(
          total = sum(!!col)
        ) %>% 
          filter(
            total > 0
          ) %>%
            nrow()
    
    print('Percentage fulfilling criteria by episode: ')
    print(criteria_total / total * 100)
  }
    
  else {
    stop("Type must be one of 'event' or 'episode'")
  }
}
```

# K-means Clustering Using Casualties & Damage

```{r Combined Metrics by Event ID}
# Create combined metrics by combining direct and indirect
details <- details %>% 
  mutate(
    attributed_deaths = deaths_direct + deaths_indirect,
    attributed_injuries = injuries_direct + injuries_indirect,
    attributed_casualties = attributed_deaths + attributed_injuries
  )
```

```{r Feature Selection & Engineering}
# Select only relevant data and remove rows with NA
model_data <- details %>% 
  select(
    event_id, attributed_casualties, damage_total, event_type
  ) %>% 
    filter(
      !is.na(damage_total) & !is.na(attributed_casualties)
    )

# Z-Score normalise the data
model_data_scaled <- model_data %>%
  mutate(
    damage_total = as.vector(scale(damage_total)),
    attributed_casualties = as.vector(scale(attributed_casualties))
  )
```

```{r Elbow Plot for Optimisation, eval=FALSE}
# Initialise empty lists
k_list = list()
sse_list = list()

# Loop through 1-30 clusters and store total sum of squared error
for (k in 1:30) {
  kmeans_spec <- k_means(num_clusters = k)
  
  kmeans_fit <- kmeans_spec %>%
    fit(~ damage_total + attributed_casualties, data = model_data_scaled)
  
  kmeans_result <- kmeans_fit$fit
  sse <- kmeans_result$tot.withinss
  
  k_list <- c(k_list, k)
  sse_list <- c(sse_list, sse)
  
  print(paste("Number of clusters:", k))
  print(paste("Total SSE:", sse))
}

# Convert lists to a DataFrame
elbow_data <- data.frame(unlist(k_list), unlist(sse_list))
names(elbow_data) = c("k","total_sse")


# Create elbow plot using clustering data
elbow_data %>% 
  ggplot(
    mapping=aes(
      x=k,
      y=total_sse
    )
  ) + 
  geom_line() +
  geom_point() + 
  scale_x_continuous(breaks = seq(1, 30, by = 1)) +
  labs(
    title='Total SSE by Number of Clusters', 
    x='Number of Clusters (k)',
    y='Total SSE'
  ) +
  theme_minimal()
```

```{r Optimised Model}
kmeans_spec <- k_means(num_clusters = 3)

kmeans_fit <- kmeans_spec %>%
  fit(~ damage_total + attributed_casualties, data = model_data_scaled)

model_data_scaled <- kmeans_fit %>% 
  augment(model_data_scaled)
```

```{r Scatter Plot of Scaled Data by Cluster}
model_data_scaled %>%
  ungroup() %>% 
    ggplot(
      mapping=aes(
        x=damage_total,
        y=attributed_casualties,
        colour=.pred_cluster
      )
    ) +
    geom_jitter() +
    labs(
      x='Scaled Total Damage',
      y='Scaled Attributed Deaths',
      title='K-Means Clustering of Casualties & Damages',
      colour='Cluster'
    ) +
    theme_minimal()
```

```{r Cluster Frequency Table}
model_data_scaled %>% 
  group_by(
    .pred_cluster
  ) %>% 
    summarise(
      total = n()
    )
```

# Change in Event Types Over Time

```{r Total Events by Type & Year}
event_data <- details %>% 
  select(
    begin_dt, event_type
  ) %>% 
    group_by(
      event_type, year(begin_dt)
    ) %>%
      summarise(
        total = n()
      ) %>% 
        arrange(
          desc(total)
        ) %>% 
          rename(
            year = `year(begin_dt)`
          )
```

```{r Scaled Largest Changes, eval=FALSE}
mr_events <- c('Drought', 'Hurricane', 'Thunderstorm Wind')

event_data %>% 
  filter(
    event_type %in% mr_events
  ) %>% 
    group_by(
      event_type
    ) %>% 
      mutate(
        mean_total = mean(total)
      ) %>% 
        ggplot(mapping = aes(
          x = year,
          y = log10(total),
          colour = reorder(event_type, desc(mean_total))
        )) + 
        geom_smooth(method = 'gam', se = FALSE) +
        scale_x_continuous(breaks = seq(1996, 2023, by = 2)) +
        labs(
          x = 'Year',
          y = 'Log10(Total Events)',
          title = 'Log10-Transformed Number of Events per Year by Event Type',
          subtitle = 'Only events with the five largest ranges of events per year are shown',
          colour = 'Event Type'
        ) +
        theme_minimal()
```

# Geospatial Correlations Between Total Events & Time by Event Type

```{r Map Correlations Between Number of Events & Time}
map_correlations <- function(event_type) {
  event_type_expr <- enquo(event_type)
  
total_region_data <- details %>%
  select(
    begin_dt, event_type, region
  ) %>% 
    group_by(
      region, year(begin_dt)
    ) %>%
      summarise(
        ov_total = n()
      ) %>% 
        rename(
          year = `year(begin_dt)`
        )

  region_data <- details %>%
    filter(
      event_type == !!event_type_expr
    ) %>% 
      select(
        begin_dt, event_type, region
      ) %>% 
        group_by(
          region, year(begin_dt)
        ) %>%
          summarise(
            total = n()
          ) %>% 
            arrange(
              desc(total)
            ) %>% 
              rename(
                year = `year(begin_dt)`
              ) %>% 
                inner_join(
                  total_region_data, by=c('region', 'year')
                ) %>% 
                  mutate(
                    prop = total / ov_total
                  )

  correlation_data <- region_data %>%
    group_by(
      region
    ) %>%
      summarise(
        corr_test = list(tryCatch(
          cor.test(
            year, 
            prop, 
            method = "pearson", 
            use = "complete.obs"
          ), 
          error = function(e) NULL
        ))
      ) %>%
        mutate(
          corr = sapply(corr_test, function(test) if (!is.null(test)) test$estimate else NA),
          p_value = sapply(corr_test, function(test) if (!is.null(test)) test$p.value else NA)
        ) %>%
          filter(
            !is.na(corr) & !is.na(p_value) & p_value <= 0.05
          ) %>%
            select(
              region, corr, p_value
            ) %>%
              mutate(
                event_type = event_type
              )
  
  map_data <- usmap::us_map(regions = "states")
  
  map_data <- map_data %>% 
    mutate(
      full = tolower(full)
    ) %>% 
      rename(
        region = full
      )
  
  correlation_data <- correlation_data %>% 
    mutate(
      region = tolower(region)
    )
  
  data <- left_join(map_data, correlation_data, by='region')
  
  plot_usmap(
    data=data, 
    values='corr'
  ) +
  scale_fill_continuous(name = "Correlation \nCoefficient") +
  theme(legend.position = "right") +
  labs(
    title=paste('Correlations Between Year &', event_type, 'Events as a Proportion of Total State Events'
          ),
    subtitle = 'Only statistically significant (p<=0.05) correlation coefficients are shown'
  )
}
```

# Deadly Storm Event Predictor

```{r}
details <- details %>% 
  mutate(
    deadly = case_when(
      deaths_direct > 0 | deaths_indirect > 0 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    )
  )

model_data <- details %>% 
  select(
    begin_dt, region, event_type, deadly
  )

model_data <- model_data %>% 
  sample_n(
    1000000
  )
```

```{r}
set.seed(1)
storm_split <- initial_split(model_data)
storm_train <- training(storm_split)
storm_test  <- testing(storm_split)
```

```{r}
storm_rec_1 <- recipe(
  deadly ~ .,
  data = storm_train
) %>% 
  step_dummy(all_nominal(), -all_outcomes())

storm_mod_1 <- logistic_reg() %>%
  set_engine("glm") %>% 
  set_mode("classification")
  
storm_wflow_1 <- workflow() %>%
  add_recipe(storm_rec_1) %>%
  add_model(storm_mod_1)
```

```{r}
library(doParallel)
registerDoParallel(cores = detectCores() - 1)

storm_fit_1 <- fit(storm_wflow_1, data = storm_train)
```

```{r}
storm_pred <- predict(
  storm_fit_1, 
  storm_test, 
  type = "prob"
) %>%
  bind_cols(
    storm_test
  )
```

```{r}
storm_pred <- storm_pred %>% 
  mutate(
    deadly = as.factor(deadly)
  )

storm_pred %>%
  roc_curve(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  ) %>%
  autoplot() +
  labs(
    title = 'ROC Curve for Deadly Storm Predictor',
    x = 'False Positive Rate / 1 - Specificity',
    y = 'True Positive Rate / Sensitivity'
  ) +
  geom_text(
    x = 0.6,
    y = 0.45,
    label = 'AUC = 0.87'
  )

```

```{r}
auc_score <- storm_pred %>% 
  roc_auc(
    truth = deadly,
    .pred_Deadly,
    event_level = "first"
  )

auc_score
```

```{r}
storm_pred <- storm_pred %>% 
  mutate(
    prediction = case_when(
      .pred_Deadly >= 0.5 ~ 'Deadly',
      TRUE ~ 'Not Deadly'
    ),
    prediction = as.factor(prediction)
  )

conf_matrix <- conf_mat(
  data = storm_pred,
  truth = deadly,
  estimate = prediction
)

confusion <- tibble(
  truth = c('Deadly', 'Not Deadly', 'Deadly', 'Not Deadly'),
  prediction = c('Deadly', 'Not Deadly', 'Not Deadly', 'Deadly'),
  n = c(160, 248085, 1666, 89)
)
```

```{r}
confusion %>% 
  ggplot(
    mapping = aes(
      x = truth, 
      y = prediction
    )
  ) +
  geom_tile(fill = "steelblue",
            color = "black",
            size = 0.5
  ) +
  geom_text(
    mapping = aes(
      label = n
    ), 
    color = "black", 
    size = 5
  ) +
  theme_minimal() +
  labs(
    title = "Confusion Matrix for Deadly Storm Predictor",
    x = "Model Prediction", 
    y = "Truth"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1), 
    legend.position = 'none'
  )
```

```{r}
test <- storm_fit_1 %>% 
  tidy()
```

```{r}
storm_pred %>% 
  mutate(
    result = case_when(
      deadly == prediction ~ 'Correct',
      TRUE ~ 'Incorrect'
    )
  ) %>% 
    group_by(
      result
    ) %>% 
      summarise(
        percentage = n()/250000*100
      )
```

